
#!/usr/bin/env python3
"""
Framework independiente de scraping para RbxServers
Obtiene servidores VIP y los env√≠a directamente a la API de Vercel
Sin necesidad de ejecutar el bot de Discord completo
"""

import asyncio
import json
import logging
import time
import requests
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Dict
import random
import sys
import os

# Selenium imports
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('standalone_scraper.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class StandaloneScraper:
    """Framework independiente de scraping para RbxServers"""
    
    def __init__(self, game_id: str = "109983668079237"):
        self.game_id = game_id
        self.api_url = "https://v0-discord-bot-api-snowy.vercel.app/api/data"
        self.cookies_file = "roblox_cookies.json"
        self.roblox_cookies = {}
        self.scraped_servers = []
        
        # Estad√≠sticas de scraping
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_servers_found': 0,
            'successful_extractions': 0,
            'failed_extractions': 0,
            'api_send_success': False,
            'duration': 0
        }
        
        logger.info(f"üöÄ Framework de scraping independiente inicializado para juego {game_id}")
        self.load_roblox_cookies()

    def load_roblox_cookies(self):
        """Cargar cookies de Roblox desde archivo"""
        try:
            if Path(self.cookies_file).exists():
                with open(self.cookies_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.roblox_cookies = data.get('cookies', {})
                    logger.info(f"‚úÖ Cargadas cookies de Roblox para {len(self.roblox_cookies)} dominios")
            else:
                logger.warning(f"‚ö†Ô∏è Archivo de cookies {self.cookies_file} no encontrado")
                self.roblox_cookies = {}
                
                # Intentar cargar desde Cookiesnew.md
                self.extract_cookies_from_cookiesnew()
        except Exception as e:
            logger.error(f"‚ùå Error cargando cookies: {e}")
            self.roblox_cookies = {}

    def extract_cookies_from_cookiesnew(self):
        """Extraer cookies del archivo Cookiesnew.md"""
        try:
            cookiesnew_path = Path("Cookiesnew.md")
            if not cookiesnew_path.exists():
                logger.warning("‚ö†Ô∏è Archivo Cookiesnew.md no encontrado")
                return 0

            with open(cookiesnew_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Buscar patrones de cookies de Roblox en el contenido
            cookie_pattern = r'_\|WARNING:.*?\|_([A-Za-z0-9]+\.[A-Za-z0-9\-_]+)'
            import re
            cookie_matches = re.findall(cookie_pattern, content)

            if cookie_matches:
                # Usar la primera cookie encontrada
                roblox_cookie = cookie_matches[0]
                
                # Inicializar estructura de cookies
                self.roblox_cookies = {
                    'roblox.com': {
                        '.ROBLOSECURITY': {
                            'value': roblox_cookie,
                            'domain': '.roblox.com',
                            'path': '/',
                            'secure': True,
                            'httpOnly': True,
                            'sameSite': 'Lax',
                            'extracted_at': datetime.now().isoformat(),
                            'source': 'Cookiesnew.md',
                            'username': 'extracted_user'
                        }
                    }
                }
                
                # Guardar cookies extra√≠das
                self.save_roblox_cookies()
                logger.info(f"‚úÖ Cookie extra√≠da de Cookiesnew.md y guardada")
                return 1
            else:
                logger.warning("‚ö†Ô∏è No se encontraron cookies v√°lidas en Cookiesnew.md")
                return 0

        except Exception as e:
            logger.error(f"‚ùå Error extrayendo cookies de Cookiesnew.md: {e}")
            return 0

    def save_roblox_cookies(self):
        """Guardar cookies a archivo"""
        try:
            cookies_data = {
                'cookies': self.roblox_cookies,
                'last_updated': datetime.now().isoformat(),
                'total_domains': len(self.roblox_cookies)
            }
            
            with open(self.cookies_file, 'w', encoding='utf-8') as f:
                json.dump(cookies_data, f, indent=2)
            logger.info(f"‚úÖ Cookies guardadas en {self.cookies_file}")
            
        except Exception as e:
            logger.error(f"‚ùå Error guardando cookies: {e}")

    def create_driver(self, retry_count: int = 0, max_retries: int = 5):
        """Crear driver de Chrome optimizado con manejo robusto de errores de Selenium"""
        driver = None
        try:
            logger.info(f"Creating Chrome driver... (Attempt {retry_count + 1}/{max_retries + 1})")
            
            # Limpiar procesos Chrome previos si es necesario
            if retry_count > 0:
                try:
                    import subprocess
                    subprocess.run(['pkill', '-f', 'chrome'], capture_output=True, timeout=5)
                    time.sleep(1)
                except:
                    pass
            
            chrome_options = Options()
            
            # Configuraci√≥n headless robusta
            chrome_options.add_argument("--headless=new")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--disable-logging")
            chrome_options.add_argument("--disable-crash-reporter")
            chrome_options.add_argument("--disable-in-process-stack-traces")
            chrome_options.add_argument("--disable-system-font-check")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # Argumentos adicionales para estabilidad en entornos de hosting
            chrome_options.add_argument("--disable-background-timer-throttling")
            chrome_options.add_argument("--disable-backgrounding-occluded-windows")
            chrome_options.add_argument("--disable-renderer-backgrounding")
            chrome_options.add_argument("--disable-features=TranslateUI,VizDisplayCompositor")
            chrome_options.add_argument("--disable-default-apps")
            chrome_options.add_argument("--no-first-run")
            chrome_options.add_argument("--disable-component-update")
            chrome_options.add_argument("--disable-background-networking")
            chrome_options.add_argument("--disable-sync")
            chrome_options.add_argument("--metrics-recording-only")
            chrome_options.add_argument("--no-report-upload")
            
            # Configuraci√≥n optimizada para velocidad
            prefs = {
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values.notifications": 2,
                "profile.managed_default_content_settings.stylesheets": 2,
                "profile.managed_default_content_settings.cookies": 1,
                "profile.managed_default_content_settings.javascript": 1,
                "profile.managed_default_content_settings.plugins": 2,
                "profile.managed_default_content_settings.popups": 2,
                "profile.managed_default_content_settings.geolocation": 2,
                "profile.managed_default_content_settings.media_stream": 2
            }
            chrome_options.add_experimental_option("prefs", prefs)
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # Crear driver con manejo mejorado de errores
            driver = webdriver.Chrome(options=chrome_options)
            
            # Configurar timeouts m√°s conservadores
            driver.set_page_load_timeout(45)
            driver.implicitly_wait(15)
            
            # Probar navegaci√≥n b√°sica con manejo de errores
            try:
                driver.get("about:blank")
                # Verificar que el driver responde
                _ = driver.title
                logger.info(f"Chrome driver created and tested successfully on attempt {retry_count + 1}")
                return driver
            except Exception as nav_error:
                logger.warning(f"Driver created but navigation test failed: {nav_error}")
                if driver:
                    try:
                        driver.quit()
                    except:
                        pass
                raise nav_error
            
        except (WebDriverException, Exception) as e:
            error_msg = str(e).lower()
            logger.warning(f"Driver creation failed on attempt {retry_count + 1}: {e}")
            
            # Limpiar driver si se cre√≥ parcialmente
            if driver:
                try:
                    driver.quit()
                except:
                    pass
            
            # Decidir si reintentar basado en el tipo de error
            should_retry = (
                retry_count < max_retries and 
                any(keyword in error_msg for keyword in ['chrome', 'selenium', 'webdriver', 'connection', 'timeout'])
            )
            
            if should_retry:
                wait_time = min(3 * (retry_count + 1), 10)  # Backoff exponencial limitado
                logger.info(f"Selenium error detected, waiting {wait_time}s before retry...")
                time.sleep(wait_time)
                return self.create_driver(retry_count + 1, max_retries)
            else:
                logger.error(f"Failed to create driver after {max_retries + 1} attempts")
                raise Exception(f"Could not create Chrome driver after {max_retries + 1} attempts: {e}")

    def load_cookies_to_driver(self, driver):
        """Cargar cookies de Roblox al driver"""
        try:
            if 'roblox.com' not in self.roblox_cookies or not self.roblox_cookies['roblox.com']:
                logger.warning("‚ö†Ô∏è No hay cookies de Roblox disponibles")
                return 0

            # Navegar a Roblox primero
            logger.info("üåê Navegando a roblox.com para aplicar cookies...")
            driver.get("https://www.roblox.com")
            time.sleep(2)

            cookies_loaded = 0
            for cookie_name, cookie_data in self.roblox_cookies['roblox.com'].items():
                try:
                    cookie_dict = {
                        'name': cookie_name,
                        'value': cookie_data['value'],
                        'domain': cookie_data.get('domain', '.roblox.com'),
                        'path': cookie_data.get('path', '/'),
                        'secure': cookie_data.get('secure', True),
                        'httpOnly': cookie_data.get('httpOnly', True)
                    }
                    
                    driver.add_cookie(cookie_dict)
                    cookies_loaded += 1
                    logger.info(f"‚úÖ Cookie aplicada: {cookie_name}")
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error aplicando cookie {cookie_name}: {e}")

            if cookies_loaded > 0:
                logger.info("üîÑ Refrescando p√°gina para aplicar cookies...")
                driver.refresh()
                time.sleep(3)

            logger.info(f"üç™ {cookies_loaded} cookies aplicadas exitosamente")
            return cookies_loaded
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando cookies al driver: {e}")
            return 0

    def get_server_links(self, driver, max_servers: int = 20) -> List[str]:
        """Obtener enlaces de servidores desde rbxservers.xyz"""
        try:
            url = f"https://rbxservers.xyz/games/{self.game_id}"
            logger.info(f"üîç Obteniendo enlaces de servidores de: {url}")
            
            driver.get(url)
            
            # Esperar a que carguen los enlaces
            wait = WebDriverWait(driver, 20)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "a[href^='/servers/']")))
            
            # Encontrar todos los enlaces de servidores
            server_elements = driver.find_elements(By.CSS_SELECTOR, "a[href^='/servers/']")
            server_links = []
            
            for element in server_elements:
                link = element.get_attribute("href")
                if link and link not in server_links:
                    server_links.append(link)
                    
                if len(server_links) >= max_servers:
                    break
            
            logger.info(f"‚úÖ Encontrados {len(server_links)} enlaces de servidores")
            return server_links
            
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo enlaces de servidores: {e}")
            return []

    def extract_vip_link(self, driver, server_url: str) -> Optional[str]:
        """Extraer link VIP de una p√°gina espec√≠fica de servidor"""
        try:
            logger.debug(f"üîç Extrayendo VIP de: {server_url}")
            
            driver.get(server_url)
            
            # Buscar el input con el link VIP
            wait = WebDriverWait(driver, 15)
            vip_input = wait.until(
                EC.presence_of_element_located(
                    (By.XPATH, "//input[@type='text' and contains(@value, 'https://')]")
                )
            )
            
            vip_link = vip_input.get_attribute("value")
            
            if vip_link and vip_link.startswith("https://www.roblox.com/games/"):
                logger.debug(f"‚úÖ VIP link extra√≠do: {vip_link[:50]}...")
                return vip_link
            else:
                logger.debug(f"‚ö†Ô∏è Link inv√°lido encontrado: {vip_link}")
                return None
                
        except TimeoutException:
            logger.debug(f"‚è∞ Timeout - No se encontr√≥ VIP link en {server_url}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Error extrayendo VIP link de {server_url}: {e}")
            return None

    async def scrape_servers(self, target_amount: int = 10, retry_count: int = 0, max_retries: int = 2) -> List[str]:
        """Ejecutar scraping completo de servidores con reintentos autom√°ticos"""
        try:
            self.stats['start_time'] = time.time()
            logger.info(f"Starting independent scraping for {target_amount} servers of game {self.game_id} (Attempt {retry_count + 1}/{max_retries + 1})")
            
            # Crear driver con reintentos autom√°ticos
            driver = self.create_driver()
            
            try:
                # Cargar cookies de Roblox
                cookies_loaded = self.load_cookies_to_driver(driver)
                logger.info(f"üç™ {cookies_loaded} cookies de Roblox aplicadas")
                
                # Obtener enlaces de servidores
                server_links = self.get_server_links(driver, target_amount * 2)  # Obtener extras por si fallan
                
                if not server_links:
                    logger.error("‚ùå No se encontraron enlaces de servidores")
                    return []
                
                self.stats['total_servers_found'] = len(server_links)
                
                # Extraer VIP links
                extracted_servers = []
                processed = 0
                
                for server_url in server_links:
                    if len(extracted_servers) >= target_amount:
                        logger.info(f"‚úÖ Meta alcanzada: {target_amount} servidores extra√≠dos")
                        break
                    
                    try:
                        processed += 1
                        logger.info(f"üîç Procesando servidor {processed}/{len(server_links)}")
                        
                        # Intentar extraer VIP link con reintentos robustos para errores de Selenium
                        vip_link = None
                        extraction_attempts = 0
                        max_extraction_attempts = 3
                        
                        while extraction_attempts < max_extraction_attempts and not vip_link:
                            try:
                                vip_link = self.extract_vip_link(driver, server_url)
                                if vip_link:
                                    break
                            except (WebDriverException, TimeoutException) as selenium_error:
                                extraction_attempts += 1
                                error_msg = str(selenium_error).lower()
                                
                                if extraction_attempts < max_extraction_attempts:
                                    wait_time = extraction_attempts  # 1s, 2s, 3s...
                                    logger.warning(f"Selenium error on extraction attempt {extraction_attempts}, waiting {wait_time}s before retry...")
                                    
                                    # Si es un error cr√≠tico de Chrome, reiniciar driver
                                    if any(keyword in error_msg for keyword in ['chrome', 'session', 'disconnected', 'crashed']):
                                        logger.warning("Critical Chrome error detected, may need driver restart")
                                        try:
                                            # Intentar recuperar el driver
                                            driver.get("about:blank")
                                        except:
                                            logger.error("Driver appears to be broken, cannot continue")
                                            break
                                    
                                    time.sleep(wait_time)
                                else:
                                    logger.error(f"Failed to extract after {max_extraction_attempts} attempts: {selenium_error}")
                            except Exception as other_error:
                                logger.error(f"Non-Selenium error during extraction: {other_error}")
                                break
                        
                        if vip_link and vip_link not in extracted_servers:
                            extracted_servers.append(vip_link)
                            self.stats['successful_extractions'] += 1
                            logger.info(f"Server {len(extracted_servers)}/{target_amount} extracted")
                        else:
                            self.stats['failed_extractions'] += 1
                            
                        # Pausa entre extracciones
                        time.sleep(random.uniform(1, 3))
                        
                    except Exception as e:
                        self.stats['failed_extractions'] += 1
                        logger.error(f"‚ùå Error procesando {server_url}: {e}")
                        continue
                
                self.scraped_servers = extracted_servers
                logger.info(f"üìä Scraping completado: {len(extracted_servers)} servidores extra√≠dos de {processed} procesados")
                
                return extracted_servers
                
            finally:
                # Cerrar driver
                try:
                    driver.quit()
                    logger.info("üîí Driver cerrado exitosamente")
                except:
                    pass
                    
        except (WebDriverException, Exception) as e:
            logger.error(f"Critical error in scraping attempt {retry_count + 1}: {e}")
            
            # Si es un error de Selenium y a√∫n tenemos reintentos disponibles
            if retry_count < max_retries and ("selenium" in str(e).lower() or "chrome" in str(e).lower() or "webdriver" in str(e).lower()):
                logger.info(f"Selenium error detected, retrying... ({retry_count + 1}/{max_retries})")
                time.sleep(3)  # Pausa antes del reintento
                return await self.scrape_servers(target_amount, retry_count + 1, max_retries)
            
            return []
        finally:
            self.stats['end_time'] = time.time()
            if self.stats['start_time']:
                self.stats['duration'] = self.stats['end_time'] - self.stats['start_time']

    async def get_game_info(self) -> Dict:
        """Obtener informaci√≥n del juego desde la API de Roblox"""
        try:
            url = f"https://games.roblox.com/v1/games?universeIds={self.game_id}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                games = data.get('data', [])
                if games:
                    game_info = games[0]
                    logger.info(f"üéÆ Informaci√≥n del juego obtenida: {game_info.get('name', 'Sin nombre')}")
                    return game_info
            
            logger.warning(f"‚ö†Ô∏è No se pudo obtener info del juego {self.game_id}")
            return {
                'name': f'Juego {self.game_id}',
                'id': self.game_id,
                'description': 'Informaci√≥n no disponible'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error obteniendo info del juego: {e}")
            return {
                'name': f'Juego {self.game_id}',
                'id': self.game_id,
                'description': 'Error obteniendo informaci√≥n'
            }

    async def send_to_vercel_api(self, servers: List[str]) -> tuple[bool, Dict]:
        """Enviar servidores a la API de Vercel"""
        try:
            logger.info(f"üåê Enviando {len(servers)} servidores a API de Vercel: {self.api_url}")
            
            # Obtener informaci√≥n del juego
            game_info = await self.get_game_info()
            
            # Preparar datos para la API
            api_data = {
                "game_name": game_info.get('name', f'Juego {self.game_id}'),
                "game_id": self.game_id,
                "total_servers": len(servers),
                "scraped_at": datetime.now().isoformat(),
                "scraped_by": {
                    "system": "standalone_scraper",
                    "framework_version": "1.0",
                    "source": "independent_framework"
                },
                "servers": servers,
                "scraping_stats": self.stats,
                "metadata": {
                    "framework": "StandaloneScraper",
                    "automated": True,
                    "cookies_used": len(self.roblox_cookies.get('roblox.com', {})),
                    "extraction_success_rate": f"{(self.stats['successful_extractions'] / max(self.stats['total_servers_found'], 1)) * 100:.1f}%"
                }
            }
            
            # Headers para la petici√≥n
            headers = {
                'Content-Type': 'application/json',
                'User-Agent': 'RbxServers-StandaloneScraper/1.0',
                'X-Framework': 'standalone'
            }
            
            # Hacer petici√≥n a la API
            response = requests.post(
                self.api_url,
                json=api_data,
                headers=headers,
                timeout=30
            )
            
            if response.status_code == 200:
                try:
                    response_data = response.json()
                except:
                    response_data = {"raw_response": response.text}
                
                logger.info(f"‚úÖ Datos enviados exitosamente a API de Vercel: {response.status_code}")
                self.stats['api_send_success'] = True
                return True, response_data
            else:
                logger.error(f"‚ùå API respondi√≥ con error: {response.status_code} - {response.text}")
                return False, {"error": f"HTTP {response.status_code}", "response": response.text}
                
        except requests.Timeout:
            logger.error(f"‚ùå Timeout enviando a API de Vercel")
            return False, {"error": "Timeout", "message": "La API no respondi√≥ en 30 segundos"}
        except Exception as e:
            logger.error(f"‚ùå Error enviando a API de Vercel: {e}")
            return False, {"error": str(e), "type": type(e).__name__}

    def save_results(self, servers: List[str]):
        """Guardar resultados localmente"""
        try:
            results_data = {
                'game_id': self.game_id,
                'scraped_at': datetime.now().isoformat(),
                'total_servers': len(servers),
                'servers': servers,
                'stats': self.stats,
                'framework': 'StandaloneScraper'
            }
            
            results_file = f"standalone_results_{self.game_id}_{int(time.time())}.json"
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results_data, f, indent=2, ensure_ascii=False)
                
            logger.info(f"üíæ Resultados guardados en: {results_file}")
            
        except Exception as e:
            logger.error(f"‚ùå Error guardando resultados: {e}")

    async def run_full_scraping(self, target_servers: int = 10):
        """Ejecutar proceso completo de scraping y env√≠o a API"""
        try:
            logger.info("=" * 60)
            logger.info(f"üöÄ INICIANDO FRAMEWORK INDEPENDIENTE DE SCRAPING")
            logger.info(f"üéÆ Juego: {self.game_id}")
            logger.info(f"üéØ Meta: {target_servers} servidores")
            logger.info(f"üåê API destino: {self.api_url}")
            logger.info("=" * 60)
            
            # Paso 1: Scraping
            logger.info("üì° PASO 1: Ejecutando scraping de servidores...")
            scraped_servers = await self.scrape_servers(target_servers)
            
            if not scraped_servers:
                logger.error("‚ùå No se obtuvieron servidores. Terminando proceso.")
                return False
            
            logger.info(f"‚úÖ PASO 1 COMPLETADO: {len(scraped_servers)} servidores obtenidos")
            
            # Paso 2: Env√≠o a API
            logger.info("üåê PASO 2: Enviando datos a API de Vercel...")
            success, response_data = await self.send_to_vercel_api(scraped_servers)
            
            if success:
                logger.info("‚úÖ PASO 2 COMPLETADO: Datos enviados exitosamente a Vercel")
            else:
                logger.error(f"‚ùå PASO 2 FALLIDO: Error enviando a API - {response_data}")
            
            # Paso 3: Guardar resultados localmente
            logger.info("üíæ PASO 3: Guardando resultados localmente...")
            self.save_results(scraped_servers)
            logger.info("‚úÖ PASO 3 COMPLETADO: Resultados guardados")
            
            # Resumen final
            logger.info("=" * 60)
            logger.info("üìä RESUMEN FINAL DEL FRAMEWORK INDEPENDIENTE")
            logger.info(f"üéÆ Juego procesado: {self.game_id}")
            logger.info(f"‚è±Ô∏è Duraci√≥n total: {self.stats['duration']:.1f} segundos")
            logger.info(f"üîç Servidores encontrados: {self.stats['total_servers_found']}")
            logger.info(f"‚úÖ Extracciones exitosas: {self.stats['successful_extractions']}")
            logger.info(f"‚ùå Extracciones fallidas: {self.stats['failed_extractions']}")
            logger.info(f"üåê Env√≠o a API: {'‚úÖ Exitoso' if self.stats['api_send_success'] else '‚ùå Fallido'}")
            logger.info(f"üìà Tasa de √©xito: {(self.stats['successful_extractions'] / max(self.stats['total_servers_found'], 1)) * 100:.1f}%")
            logger.info("=" * 60)
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en proceso completo: {e}")
            return False

async def main():
    """Funci√≥n principal del framework independiente"""
    try:
        # Configuraci√≥n
        GAME_ID = "109983668079237"  # Juego por defecto
        TARGET_SERVERS = 15  # Cantidad de servidores a obtener
        
        # Permitir configuraci√≥n por argumentos de l√≠nea de comandos
        if len(sys.argv) > 1:
            GAME_ID = sys.argv[1]
        if len(sys.argv) > 2:
            try:
                TARGET_SERVERS = int(sys.argv[2])
            except ValueError:
                logger.warning(f"‚ö†Ô∏è Cantidad inv√°lida: {sys.argv[2]}, usando {TARGET_SERVERS}")
        
        # Crear framework
        scraper = StandaloneScraper(GAME_ID)
        
        # Ejecutar scraping completo
        success = await scraper.run_full_scraping(TARGET_SERVERS)
        
        if success:
            logger.info("üéâ Framework independiente ejecutado exitosamente")
            return 0
        else:
            logger.error("‚ùå Framework independiente fall√≥")
            return 1
            
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è Framework detenido por el usuario")
        return 0
    except Exception as e:
        logger.error(f"‚ùå Error cr√≠tico en main: {e}")
        return 1

if __name__ == "__main__":
    print("ü§ñ Framework Independiente de Scraping RbxServers")
    print("=" * 50)
    print("üìã Uso:")
    print("  python3 standalone_scraper.py [game_id] [cantidad]")
    print("üìã Ejemplo:")
    print("  python3 standalone_scraper.py 109983668079237 15")
    print("=" * 50)
    
    # Ejecutar framework
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
